# -*- coding: utf-8 -*-
"""Blood_Cancer Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15hv-M_E5DILlXX5DLpaLhWD_pz1le5Gp
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from matplotlib import pyplot
from pandas import read_csv
from pandas.plotting import scatter_matrix
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import model_selection
from sklearn.ensemble import BaggingClassifier
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import GradientBoostingClassifier
import numpy as np
from sklearn.ensemble import RandomForestClassifier
# importing utility modules
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss
# importing machine learning models for prediction
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
# importing voting classifer
from sklearn.ensemble import VotingClassifier
from sklearn.naive_bayes import GaussianNB
import numpy as np
import cv2
import os
import glob
import math
from scipy import signal
from pywt import dwt2
import skimage
from keras.applications.vgg19 import preprocess_input
from keras.applications.vgg19 import decode_predictions
from keras.applications.vgg19 import VGG19
from keras.models import Model
from pickle import dump

from builtins import range, input
from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, AveragePooling2D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from sklearn.metrics import confusion_matrix, roc_curve
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt


from builtins import range, input
from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, AveragePooling2D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from sklearn.metrics import confusion_matrix, roc_curve
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

import numpy as np
import cv2
import os
import glob
import math
from scipy import signal
from pywt import dwt2
import skimage
from keras.applications.vgg19 import decode_predictions
import tensorflow as tf
from tensorflow.keras.utils import load_img
from tensorflow.keras.utils import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.models import Model
from pickle import dump
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
from keras.applications.inception_v3 import InceptionV3
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.svm import SVC

from niapy.problems import Problem
from niapy.task import Task
from niapy.algorithms.basic import ParticleSwarmOptimization
from niapy.algorithms.basic import CatSwarmOptimization
from sklearn.model_selection import cross_val_score,KFold
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import learning_curve
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
# all imports
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.datasets import  make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

"""**Dataset Loading**"""

dataset = pd.read_csv("/content/drive/MyDrive/01. CSV_files/02. Blood_Cancer /Merged4Class_ResNet50.CSV")
array = dataset.values
X = array[:,0:2048]
Y = array[:,2048]
target_names = ['class 0', 'class 1', 'class 2', 'class 3']

dataset.isnull().values.any()

"""**Import Models**"""

model1 = SVC()
model2 = RandomForestClassifier()
model3 = DecisionTreeClassifier()
model4 = GaussianNB()
model5 = XGBClassifier()
model6 = KNeighborsClassifier()
model7 = LogisticRegression()

"""**Experimental Data Analysis**"""

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)

model7.fit(X_train, y_train)
prediction = model7.predict(X_test)
print("======================Score===================")
#print(classification_report(y_test,prediction))
print('Accuracy: %.4f' % accuracy_score(y_test, prediction))
print('Precision: %.4f' % precision_score(y_test, prediction, average='macro'))
print('Recall: %.4f' % recall_score(y_test, prediction, average='macro'))
print('F1 Score: %.4f' % f1_score(y_test, prediction, average='macro'))

for i in range(5):
 kf=KFold(n_splits=2+i)
 score=cross_val_score(model7, X_test, y_test,cv=kf)
 print("\n--------------------------------------")
 print("Cross Validation Scores are {}".format(score))
 print("Average Cross Validation score :{}".format(score.mean()))
 print("\n--------------------------------------")

"""**SVC Feature Selection for PSO**"""

#Define Class
class SVMFeatureSelection(Problem):
    def __init__(self, X_train, y_train, alpha=0.99):
        super().__init__(dimension=X_train.shape[1], lower=0, upper=1)
        self.X_train = X_train
        self.y_train = y_train
        self.alpha = alpha

    def _evaluate(self, x):
        selected = x > 0.5
        num_selected = selected.sum()
        if num_selected == 0:
            return 1.0
        accuracy = cross_val_score(SVC(), self.X_train[:, selected], self.y_train, cv=2, n_jobs=-1).mean()
        score = 1 - accuracy
        num_features = self.X_train.shape[1]
        return self.alpha * score + (1 - self.alpha) * (num_selected / num_features)

"""**Dataset Split for Training and Testing with PSO**"""

from niapy.algorithms.basic import ParticleSwarmOptimization
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=1234)

"""**Multiples Model Running**"""

#-------------------Using SVM_Feature Selection for best Feature----------------
problem = SVMFeatureSelection(X_train, y_train)
task = Task(problem, max_iters=5)
algorithm = ParticleSwarmOptimization(population_size=10, seed=1234)
best_features, best_fitness = algorithm.run(task)

selected_features = best_features > 0.5
print('Number of selected features:', selected_features.sum())
#print('Selected features:', ', '.join(feature_names[selected_features].tolist()))

#-------------------Model Selection---------------------------------------------
print("\n=============SVC============")
model_selected = model7
model_all = model7
model_selected.fit(X_train[:, selected_features], y_train)
print('Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))
model_all.fit(X_train, y_train)
print('All Features Accuracy:', model_all.score(X_test, y_test))
prediction = model_all.predict(X_test)
#print(classification_report(y_test,prediction))

print('Accuracy: %.4f' % accuracy_score(y_test, prediction))
print('Precision: %.4f' % precision_score(y_test, prediction, average='macro'))
print('Recall: %.4f' % recall_score(y_test, prediction, average='macro'))
print('F1 Score: %.4f' % f1_score(y_test, prediction, average='macro'))

#-----------------------K-fold validation---------------------------------------
for i in range(5):
 kf=KFold(n_splits=2+i)
 score=cross_val_score(model7, X_test, y_test,cv=kf)
 print("\n--------------------------------------")
 print("Cross Validation Scores are {}".format(score))
 print("Average Cross Validation score :{}".format(score.mean()))
 print("\n--------------------------------------")

#--------------------Learning Curve---------------------------------------------
train_sizes, train_scores, test_scores = learning_curve(model_all, X_test, y_test, cv=10, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50))
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
plt.subplots(1, figsize=(7,5))
plt.plot(train_sizes, train_mean, '--', color="#111111",  label="Training score")
plt.plot(train_sizes, test_mean, color="orchid", label="Cross-validation score")

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color="#DDDDDD")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color="#DDDDDD")

plt.title("Learning Curve")
plt.xlabel("Training Set Size"), plt.ylabel("Accuracy Score"), plt.legend(loc="best")
plt.tight_layout()
plt.show()

#----------------------Confusion Matrix--------------------------------
def plot_confusion_matrix(normalize):
  plt.figure(figsize=(7,5))
  classes = ['Benign','Early', 'Pre', 'Pro']
  tick_marks = [0.5,1.5, 2.5, 3.5]
  cn = confusion_matrix(y_test, prediction,normalize=normalize)
  sns.heatmap(cn,cmap='gnuplot',annot=True)
  plt.xticks(tick_marks, classes)
  plt.yticks(tick_marks, classes)
  plt.title('Confusion Matrix')
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()
print('Confusion Matrix for LR with Normalized Values')
plot_confusion_matrix(normalize='true')

#----------------------Roc curves-----------------------
def plot_roc_curve(y_test, y_pred):

  n_classes = len(np.unique(y_test))
  y_test = label_binarize(y_test, classes=np.arange(n_classes))
  y_pred = label_binarize(y_pred, classes=np.arange(n_classes))

  # Compute ROC curve and ROC area for each class
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  fpr = dict()
  lw=2
  for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
  colors = cycle(['blue', 'red', 'green', 'brown'])
  for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))
  plt.plot([0, 1], [0, 1], 'k--', lw=lw)
  plt.xlim([-0.05, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver operating characteristic for multi-class data')
  plt.legend(loc="lower right")
  
  plt.show()
plot_roc_curve(y_test, prediction)

"""**Feature Selection and Reduction**"""

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=1234)

"""**LDA**"""

train_x, test_x, y_train, test_y = train_test_split(X, Y, test_size=0.2)

#lda Analysis
lda = LinearDiscriminantAnalysis()
x_train = lda.fit_transform(train_x, y_train)
x_test = lda.transform(test_x)
sc = StandardScaler()
x_train = sc.fit_transform(train_x)
x_test = sc.transform(test_x)

model1.fit(x_train,y_train)
prediction = model1.predict(x_test)
print("======================Score===================")
#print(classification_report(y_test,prediction))
print('Accuracy: %.4f' % accuracy_score(test_y, prediction))
print('Precision: %.4f' % precision_score(test_y, prediction, average='macro'))
print('Recall: %.4f' % recall_score(test_y, prediction, average='macro'))
print('F1 Score: %.4f' % f1_score(test_y, prediction, average='macro'))

"""**PCA**"""

#pca analysis
sc = StandardScaler()
x_train = sc.fit_transform(train_x)
x_test = sc.transform(test_x)

pca = PCA(n_components= 1600)
x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)

model7.fit(x_train,y_train)
prediction = model7.predict(x_test)
print("======================Score===================")
#print(classification_report(y_test,prediction))
print('Accuracy: %.4f' % accuracy_score(test_y, prediction))
print('Precision: %.4f' % precision_score(test_y, prediction, average='macro'))
print('Recall: %.4f' % recall_score(test_y, prediction, average='macro'))
print('F1 Score: %.4f' % f1_score(test_y, prediction, average='macro'))

"""**RFE**"""

from niapy.algorithms.basic import BacterialForagingOptimization

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=1234)

#-------------------Using SVM_Feature Selection for best Feature----------------
problem = SVMFeatureSelection(X_train, y_train)
task = Task(problem, max_iters=1)
algorithm = BacterialForagingOptimization(population_size=10, seed=1234)
best_features, best_fitness = algorithm.run(task)

selected_features = best_features > 0.5
print('Number of selected features:', selected_features.sum())
#print('Selected features:', ', '.join(feature_names[selected_features].tolist()))

#-------------------Model Selection---------------------------------------------
print("\n=============SVC============")
model_selected = model1
model_all = model1
model_selected.fit(X_train[:, selected_features], y_train)
print('Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))
model_all.fit(X_train, y_train)
print('All Features Accuracy:', model_all.score(X_test, y_test))
prediction = model_all.predict(X_test)
#print(classification_report(y_test,prediction))

print('Accuracy: %.4f' % accuracy_score(y_test, prediction))
print('Precision: %.4f' % precision_score(y_test, prediction, average='macro'))
print('Recall: %.4f' % recall_score(y_test, prediction, average='macro'))
print('F1 Score: %.4f' % f1_score(y_test, prediction, average='macro'))